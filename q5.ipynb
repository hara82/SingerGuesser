{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5997757847533632, 0.20011210762331838, 0.20011210762331838)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "with open('splitData.pickle', 'rb') as f:\n",
    "    texts_train = pickle.load(f)\n",
    "    texts_val = pickle.load(f)\n",
    "    texts_test = pickle.load(f)\n",
    "    labels_train = pickle.load(f)\n",
    "    labels_val = pickle.load(f)\n",
    "    labels_test = pickle.load(f)\n",
    "\n",
    "all_texts  = texts_train  + texts_val  + texts_test\n",
    "all_labels = labels_train + labels_val + labels_test\n",
    "# check the ratio of each dataset\n",
    "total = len(texts_train) + len(texts_val) + len(texts_test)\n",
    "len(texts_train)/total, len(texts_val)/total, len(texts_test)/total"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ここからコピーする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'Dua Lipa',\n",
       " 1: 'Taylor Swift',\n",
       " 2: 'Eminem',\n",
       " 3: 'Jennifer Lopez',\n",
       " 4: 'Ariana Grande',\n",
       " 5: 'Selena Gomez',\n",
       " 6: 'Michael Jackson',\n",
       " 7: 'Beyonce Knowles',\n",
       " 8: 'Lady Gaga',\n",
       " 9: 'XXXTentacion'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# correspond labels(songer names) with integers\n",
    "label2id = {label: i for i, label in enumerate(set(labels_train))}\n",
    "id2label = {id:label for label,id in label2id.items()}\n",
    "id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_train_int = [label2id[l] for l in labels_train]\n",
    "labels_val_int = [label2id[l] for l in labels_val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'datasets'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/home/denjo/Glasgow2/textAsData/q5.ipynb Cell 6\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/denjo/Glasgow2/textAsData/q5.ipynb#X10sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m AutoTokenizer\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/denjo/Glasgow2/textAsData/q5.ipynb#X10sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdatasets\u001b[39;00m \u001b[39mimport\u001b[39;00m Dataset\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/denjo/Glasgow2/textAsData/q5.ipynb#X10sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m tokenizer \u001b[39m=\u001b[39m AutoTokenizer\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m\"\u001b[39m\u001b[39mroberta-base\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/denjo/Glasgow2/textAsData/q5.ipynb#X10sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreateDataset\u001b[39m(texts, labels_int):\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'datasets'"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from datasets import Dataset\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\",)\n",
    "\n",
    "def createDataset(texts, labels_int):\n",
    "    encoded_texts = []\n",
    "    for t in texts:\n",
    "        encoded_texts.append(tokenizer.encode(t,truncation=True))\n",
    "    \n",
    "    dataset = Dataset.from_dict({\n",
    "        \"input_ids\":encoded_texts,\n",
    "        \"labels\": labels_int\n",
    "    })\n",
    "    return dataset\n",
    "\n",
    "train_dataset = createDataset(texts_train, labels_train_int)\n",
    "validation_dataset = createDataset(texts_val, labels_val_int)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import set_seed, AutoModelForSequenceClassification\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained('roberta-base',id2label=id2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "learning_rate = 1e-4\n",
    "epochs = 1\n",
    "batch_size = 16\n",
    "weight_decay = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"singers_model\", # HuggingFace wants a name for your model\n",
    "    evaluation_strategy=\"epoch\", # How often we want to evaluate the model\n",
    "    learning_rate=learning_rate, # Hyperparameter\n",
    "    per_device_train_batch_size=batch_size, # Hyperparameter\n",
    "    per_device_eval_batch_size=batch_size, # Hyperparameter\n",
    "    num_train_epochs=epochs, # Hyperparameter\n",
    "    weight_decay=weight_decay, # Hyperparameter\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model, # The model you want to train\n",
    "    args=training_args, # The various training arguments set up above\n",
    "    train_dataset=train_dataset, # The data to use to update the weights\n",
    "    eval_dataset=validation_dataset, # The data to use \n",
    "    tokenizer=tokenizer, # The tokenizer used on the data\n",
    "    data_collator=data_collator, # A data collator that does clever things moving data around\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification, AdamW\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from datasets import Dataset\n",
    "\n",
    "# Load data\n",
    "data = pd.read_csv(\"song_lyrics.csv\")\n",
    "\n",
    "# Split data into training and testing sets\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(data[\"lyrics\"], data[\"singer\"], test_size=0.2, random_state=42)\n",
    "\n",
    "# Create PyTorch dataset\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "train_dataset = Dataset.from_dict({'input_ids': tokenizer.batch_encode_plus(train_texts.tolist(), \n",
    "                                                                           padding=True, \n",
    "                                                                           truncation=True)['input_ids'],\n",
    "                                   'attention_mask': tokenizer.batch_encode_plus(train_texts.tolist(), \n",
    "                                                                                   padding=True, \n",
    "                                                                                   truncation=True)['attention_mask'],\n",
    "                                   'labels': train_labels.tolist()})\n",
    "test_dataset = Dataset.from_dict({'input_ids': tokenizer.batch_encode_plus(test_texts.tolist(), \n",
    "                                                                          padding=True, \n",
    "                                                                          truncation=True)['input_ids'],\n",
    "                                  'attention_mask': tokenizer.batch_encode_plus(test_texts.tolist(), \n",
    "                                                                                  padding=True, \n",
    "                                                                                  truncation=True)['attention_mask'],\n",
    "                                  'labels': test_labels.tolist()})\n",
    "\n",
    "# Convert to PyTorch dataset\n",
    "train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "test_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "\n",
    "# Create data loader\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=8, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "634\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Postcard (New Shoes)\\nPostcard (New Shoes) Lyrics\\nHey, honey\\nCat got ya tongue?\\nDon't look so funny\\nMr. Comedian, joke's on you and\\nYou good at bets, so where's my money?\\nI know you bet I'd never make that move\\nSo, why's the house empty?\\nLights off by the end of this week\\nI leave wit' what I came wit', baby\\nI'll take the dogs, you keep the Mercedes\\n'Cause I'm gone, I'm not fazed\\nI walk these roads with grace\\nAnd I'll wave with you behind me\\nWith a new smile on my face, 'cause\\n\\nI'm walkin' in new shoes now\\nI got a new song to sing\\nWhen I walk in the room, every head turns\\nEvery eye is on me\\nToo bad you're not here to see it\\nAnd by the time you get this\\nI'll be so long, gone and far\\nI'll send you a postcard\\nI'll send you a postcard\\n\\nHey, baby\\nYou seen a ghost?\\nThis ain't yo' lady\\nThis is the chick that you held back\\nNow, all of the things you said were crazy\\nI checked off my list and I feel good about it\\nI got the new haircut\\nThe one that you said wouldn't look no good\\nTook that job you said I could not get\\nWore that dress you said I'd never fit\\nYou might also like\\n'Cause you're gone, and I'm great\\nI got rid of your dead weight\\nAnd I'll wave at you behind me\\nWith a new smile on my face, 'cause\\n\\nI'm walkin' in new shoes now\\nI got a new song to sing\\nWhen I walk in the room, every head turns\\nEvery eye is on me\\nToo bad you're not here to see it\\nAnd by the time you get this\\nI'll be so long, gone and far\\nI'll send you a postcard\\nI'll send you a postcardEmbed\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(len(texts_train)):\n",
    "    if \"Postcard\" in texts_train[i]:\n",
    "        print(i)\n",
    "\n",
    "texts_train[634]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "why do I get the error below for the code below?\n",
    "raise ValueError(\n",
    "   2972                     f\"type of {first_element} unknown: {type(first_element)}. \"\n",
    "   2973                     \"Should be one of a python, numpy, pytorch or tensorflow object.\"\n",
    "-----------\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
    "from transformers import set_seed, AutoModelForSequenceClassification\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained('roberta-base')\n",
    "# parameters\n",
    "learning_rate = 1e-4\n",
    "epochs = 1\n",
    "batch_size = 16\n",
    "weight_decay = None\n",
    "\n",
    "\n",
    "from datasets import Dataset\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "train_dataset = Dataset.from_dict({\"input_ids\":texts_train,\"labels\":labels_train})\n",
    "validation_dataset = Dataset.from_dict({\"input_ids\":texts_val, \"labels\":labels_val})\n",
    "train_dataset[0]\n",
    "\n",
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"singers_model\", # HuggingFace wants a name for your model\n",
    "    evaluation_strategy=\"epoch\", # How often we want to evaluate the model\n",
    "    learning_rate=learning_rate, # Hyperparameter\n",
    "    per_device_train_batch_size=batch_size, # Hyperparameter\n",
    "    per_device_eval_batch_size=batch_size, # Hyperparameter\n",
    "    num_train_epochs=epochs, # Hyperparameter\n",
    "    weight_decay=weight_decay, # Hyperparameter\n",
    ")\n",
    "\n",
    "\n",
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model, # The model you want to train\n",
    "    args=training_args, # The various training arguments set up above\n",
    "    train_dataset=train_dataset, # The data to use to update the weights\n",
    "    eval_dataset=validation_dataset, # The data to use \n",
    "    tokenizer=tokenizer, # The tokenizer used on the data\n",
    "    data_collator=data_collator, # A data collator that does clever things moving data around\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
