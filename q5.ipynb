{"cells":[{"cell_type":"markdown","metadata":{"id":"z5-bkmgYXYcH"},"source":["## importing data\n"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2011,"status":"ok","timestamp":1678565450055,"user":{"displayName":"Ryosuke HARA","userId":"14758738608613309303"},"user_tz":0},"id":"YI9h-Ou-lPwL","outputId":"6ca4d47b-cb1f-43e2-edf1-113eb47bb080"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":22,"status":"ok","timestamp":1678565450056,"user":{"displayName":"Ryosuke HARA","userId":"14758738608613309303"},"user_tz":0},"id":"bHlq8svAW5m-","outputId":"d2913a72-dfaf-4465-b8e8-8deca2591701"},"outputs":[{"data":{"text/plain":["(0.5997757847533632, 0.20011210762331838, 0.20011210762331838)"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["import pickle\n","file_path = '/content/drive/My Drive/Colab Notebooks/textAsData/splitData.pickle'\n","with open(file_path, 'rb') as f:\n","    texts_train = pickle.load(f)\n","    texts_val = pickle.load(f)\n","    texts_test = pickle.load(f)\n","    labels_train = pickle.load(f)\n","    labels_val = pickle.load(f)\n","    labels_test = pickle.load(f)\n","\n","all_texts  = texts_train  + texts_val  + texts_test\n","all_labels = labels_train + labels_val + labels_test\n","# check the ratio of each dataset\n","total = len(texts_train) + len(texts_val) + len(texts_test)\n","len(texts_train)/total, len(texts_val)/total, len(texts_test)/total"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8723,"status":"ok","timestamp":1678565458769,"user":{"displayName":"Ryosuke HARA","userId":"14758738608613309303"},"user_tz":0},"id":"_S4Y1bDmXK5D","outputId":"a0e72c3d-91d8-4306-a962-5683db7988cb"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: transformers in /usr/local/lib/python3.9/dist-packages (4.26.1)\n","Requirement already satisfied: datasets in /usr/local/lib/python3.9/dist-packages (2.10.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.9.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.25.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.6.2)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.13.2)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.13.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.0)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (from datasets) (1.3.5)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.9/dist-packages (from datasets) (3.2.0)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.9/dist-packages (from datasets) (0.70.14)\n","Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.9/dist-packages (from datasets) (2023.3.0)\n","Requirement already satisfied: dill<0.3.7,>=0.3.0 in /usr/local/lib/python3.9/dist-packages (from datasets) (0.3.6)\n","Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.9/dist-packages (from datasets) (9.0.0)\n","Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.9/dist-packages (from datasets) (0.18.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.9/dist-packages (from datasets) (3.8.4)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (1.8.2)\n","Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (3.1.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (1.3.3)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (22.2.0)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (1.3.1)\n","Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (4.0.2)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (6.0.4)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.14)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (4.0.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.9/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.9/dist-packages (from pandas->datasets) (2022.7.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n"]}],"source":["!pip install transformers datasets"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"9aG6Jnk5fGYw"},"source":["# (A) Using Context Vectors for Classification"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10264,"status":"ok","timestamp":1678565469028,"user":{"displayName":"Ryosuke HARA","userId":"14758738608613309303"},"user_tz":0},"id":"vp9kLvabe6BT","outputId":"a20058fb-9b66-4734-93cf-280650075ffb"},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.weight']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]}],"source":["import torch\n","from transformers import pipeline\n","pipe = pipeline('feature-extraction',model='roberta-base',truncation=True)# Truncation is needed to handle longer documents over 512 tokens"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1980431,"status":"ok","timestamp":1678549276712,"user":{"displayName":"Ryosuke HARA","userId":"14758738608613309303"},"user_tz":0},"id":"RXCIy7a-f3kx","outputId":"566812f4-12ee-4a55-9700-65725aa684b2"},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 1070/1070 [33:00<00:00,  1.85s/it]\n"]},{"data":{"text/plain":["torch.Size([1070, 768])"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["from tqdm import tqdm\n","# create the context vector of [CLS] token of training set.\n","\n","starts_train = []\n","for i in tqdm(range(len(texts_train))):\n","  context_vector = pipe(texts_train[i],return_tensors='pt')\n","  starts_train.append(context_vector[0,0,:])\n","starts_train = torch.stack(starts_train)\n","starts_train.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":598724,"status":"ok","timestamp":1678549875396,"user":{"displayName":"Ryosuke HARA","userId":"14758738608613309303"},"user_tz":0},"id":"Vo9OZZ2rh_2f","outputId":"6fd00fe4-a643-4b4d-ad58-0abeb4157ccf"},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 357/357 [09:58<00:00,  1.68s/it]\n"]},{"data":{"text/plain":["torch.Size([357, 768])"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["# create the context vector of [CLS] token of validation set.\n","\n","starts_val = []\n","for i in tqdm(range(len(texts_val))):\n","  context_vector = pipe(texts_val[i],return_tensors='pt')\n","  starts_val.append(context_vector[0,0,:])\n","starts_val = torch.stack(starts_val)\n","starts_val.shape"]},{"cell_type":"markdown","metadata":{"id":"7dEt442AfvGf"},"source":["### Logistic Regression"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":3366,"status":"ok","timestamp":1678565047879,"user":{"displayName":"Ryosuke HARA","userId":"14758738608613309303"},"user_tz":0},"id":"RSONFE4ifue6"},"outputs":[],"source":["# Show classifier performance\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n","def showPerformance(labels_true, labels_predicted):\n","    print(f'accuracy ={accuracy_score(labels_true, labels_predicted):.3f}')\n","    print(f'precision={precision_score(labels_true, labels_predicted, average=\"macro\"):.3f}')\n","    print(f'recall   ={recall_score(labels_true, labels_predicted, average=\"macro\"):.3f}') \n","    print(f'f1       ={f1_score(labels_true, labels_predicted, average=\"macro\"):.3f}')\n","    return ;\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6357,"status":"ok","timestamp":1678550545534,"user":{"displayName":"Ryosuke HARA","userId":"14758738608613309303"},"user_tz":0},"id":"BmIte8ntffJL","outputId":"5165f748-4140-4b2b-b076-9a947df7cec9"},"outputs":[{"name":"stdout","output_type":"stream","text":["accuracy =0.524\n","precision=0.476\n","recall   =0.517\n","f1       =0.485\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]}],"source":["from sklearn.linear_model import LogisticRegression\n","\n","# Logistic Regression\n","clf = LogisticRegression(random_state=42,max_iter=1000).fit(starts_train.detach().numpy(), labels_train)\n","labels_predicted = clf.predict(starts_val.detach().numpy())\n","\n","\n","showPerformance(labels_val, labels_predicted)"]},{"cell_type":"markdown","metadata":{"id":"I1t-sWenjZkV"},"source":["# (b)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Create Dataset for Training and Validation"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1678565469029,"user":{"displayName":"Ryosuke HARA","userId":"14758738608613309303"},"user_tz":0},"id":"pnZ48Ij1ubv7","outputId":"51a53c3a-83a5-4062-b6a0-d45ed277c702"},"outputs":[],"source":["# correspond labels(songer names) with integers\n","label2id = {\"Beyonce Knowles\":0,\n","            \"Michael Jackson\":1,\n","            \"Lady Gaga\":2,\n","            \"Taylor Swift\":3,\n","            \"Ariana Grande\":4,\n","            \"Selena Gomez\":5,\n","            \"Eminem\":6,\n","            \"Jennifer Lopez\":7,\n","            \"Dua Lipa\":8,\n","            \"XXXTentacion\":9}\n","id2label = {id:label for label,id in label2id.items()}"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1678565469029,"user":{"displayName":"Ryosuke HARA","userId":"14758738608613309303"},"user_tz":0},"id":"o0IWt7Hzu2ya"},"outputs":[],"source":["labels_train_int = [label2id[l] for l in labels_train]\n","labels_val_int = [label2id[l] for l in labels_val]"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":7802,"status":"ok","timestamp":1678565476826,"user":{"displayName":"Ryosuke HARA","userId":"14758738608613309303"},"user_tz":0},"id":"tQuSJXIVu5wx"},"outputs":[],"source":["from transformers import AutoTokenizer\n","from datasets import Dataset\n","\n","tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n","\n","def createDataset(texts, labels_int):\n","    encoded_texts = []\n","    for t in texts:\n","        encoded_texts.append(tokenizer.encode(t,truncation=True))#,max_length=128))### max_length need to be changed\n","    \n","    dataset = Dataset.from_dict({\n","        \"input_ids\":encoded_texts,\n","        \"labels\": labels_int\n","    })\n","    return dataset\n","\n","train_dataset = createDataset(texts_train, labels_train_int)\n","validation_dataset = createDataset(texts_val, labels_val_int)\n"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16,"status":"ok","timestamp":1678565476828,"user":{"displayName":"Ryosuke HARA","userId":"14758738608613309303"},"user_tz":0},"id":"L1elN4NTu68l","outputId":"a88bafd5-6d00-46d5-b086-84be0f574395"},"outputs":[],"source":["# check Dataset\n","print(train_dataset)\n","train_taset[0][\"input_ids\"][:10]"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5674,"status":"ok","timestamp":1678565482493,"user":{"displayName":"Ryosuke HARA","userId":"14758738608613309303"},"user_tz":0},"id":"zR4Gd4nmvB5C","outputId":"d44fdfcd-f770-447c-9c73-b84d7fddd32c"},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.weight', 'lm_head.bias', 'lm_head.dense.weight']\n","- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["from transformers import set_seed, AutoModelForSequenceClassification\n","\n","set_seed(42)\n","\n","model = AutoModelForSequenceClassification.from_pretrained('roberta-base',id2label=id2label)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Training with Designated Parameters"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":35,"status":"ok","timestamp":1678565083211,"user":{"displayName":"Ryosuke HARA","userId":"14758738608613309303"},"user_tz":0},"id":"kU8_HdDRvU87"},"outputs":[],"source":["# parameters\n","learning_rate = 1e-4\n","epochs = 1\n","batch_size = 16\n","# weight_decay = None"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":1097,"status":"ok","timestamp":1678565084275,"user":{"displayName":"Ryosuke HARA","userId":"14758738608613309303"},"user_tz":0},"id":"IM_gwYyyvXVo"},"outputs":[],"source":["from transformers import TrainingArguments, Trainer\n","\n","training_args = TrainingArguments(\n","    output_dir=\"singers_model\", # HuggingFace wants a name for your model\n","    evaluation_strategy=\"epoch\", # How often we want to evaluate the model\n","    learning_rate=learning_rate, # Hyperparameter\n","    per_device_train_batch_size=batch_size, # Hyperparameter\n","    per_device_eval_batch_size=batch_size, # Hyperparameter\n","    num_train_epochs=epochs # Hyperparameter\n","    # weight_decay=weight_decay, # Hyperparameter\n",")"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":2586,"status":"ok","timestamp":1678565086858,"user":{"displayName":"Ryosuke HARA","userId":"14758738608613309303"},"user_tz":0},"id":"5sm180T5vZel"},"outputs":[],"source":["from transformers import DataCollatorWithPadding\n","\n","data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n","\n","\n","trainer = Trainer(\n","    model=model, # The model you want to train\n","    args=training_args, # The various training arguments set up above\n","    train_dataset=train_dataset, # The data to use to update the weights\n","    eval_dataset=validation_dataset, # The data to use \n","    tokenizer=tokenizer, # The tokenizer used on the data\n","    data_collator=data_collator, # A data collator that does clever things moving data around\n",")"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":491},"executionInfo":{"elapsed":128972,"status":"ok","timestamp":1678565215823,"user":{"displayName":"Ryosuke HARA","userId":"14758738608613309303"},"user_tz":0},"id":"LYaJxSrCvdw5","outputId":"d44b796b-ef8c-4b20-945c-2ecacb77dbd5"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","***** Running training *****\n","  Num examples = 1070\n","  Num Epochs = 1\n","  Instantaneous batch size per device = 16\n","  Total train batch size (w. parallel, distributed & accumulation) = 16\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 67\n","  Number of trainable parameters = 124653322\n","You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='67' max='67' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [67/67 02:06, Epoch 1/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>No log</td>\n","      <td>2.266755</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["***** Running Evaluation *****\n","  Num examples = 357\n","  Batch size = 16\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n"]},{"data":{"text/plain":["TrainOutput(global_step=67, training_loss=2.2934135323140157, metrics={'train_runtime': 129.0258, 'train_samples_per_second': 8.293, 'train_steps_per_second': 0.519, 'total_flos': 281549051105280.0, 'train_loss': 2.2934135323140157, 'epoch': 1.0})"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["trainer.train()"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":69},"executionInfo":{"elapsed":13377,"status":"ok","timestamp":1678565229155,"user":{"displayName":"Ryosuke HARA","userId":"14758738608613309303"},"user_tz":0},"id":"6pSe-XvmwdNZ","outputId":"58b60592-7009-404a-c081-f64266edab39"},"outputs":[{"name":"stderr","output_type":"stream","text":["***** Running Prediction *****\n","  Num examples = 357\n","  Batch size = 16\n"]},{"data":{"text/html":[],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}],"source":["predictions, label_ids, metrics = trainer.predict(validation_dataset)"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":32,"status":"ok","timestamp":1678565229156,"user":{"displayName":"Ryosuke HARA","userId":"14758738608613309303"},"user_tz":0},"id":"TEqVTG-cwq3U","outputId":"ee4d742b-e580-4ab5-a968-23e0d9bfef13"},"outputs":[{"data":{"text/plain":["{'test_loss': 2.2667553424835205,\n"," 'test_runtime': 13.34,\n"," 'test_samples_per_second': 26.762,\n"," 'test_steps_per_second': 1.724}"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["metrics"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":28,"status":"ok","timestamp":1678565229156,"user":{"displayName":"Ryosuke HARA","userId":"14758738608613309303"},"user_tz":0},"id":"gG77rx-Hwrjk","outputId":"0d64d58d-5c74-4bdb-8d55-94b829d2df04"},"outputs":[{"name":"stdout","output_type":"stream","text":["type(predictions)=<class 'numpy.ndarray'>\n","predictions.shape=(357, 10)\n"]}],"source":["print(f\"{type(predictions)=}\")\n","print(f\"{predictions.shape=}\")"]},{"cell_type":"code","execution_count":17,"metadata":{"executionInfo":{"elapsed":25,"status":"ok","timestamp":1678565229156,"user":{"displayName":"Ryosuke HARA","userId":"14758738608613309303"},"user_tz":0},"id":"ddXGtdzRPuzP"},"outputs":[],"source":["# Show classifier performance\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n","def showPerformance(labels_true, labels_predicted):\n","    print(f'accuracy ={accuracy_score(labels_true, labels_predicted):.3f}')\n","    print(f'precision={precision_score(labels_true, labels_predicted, average=\"macro\"):.3f}')\n","    print(f'recall   ={recall_score(labels_true, labels_predicted, average=\"macro\"):.3f}') \n","    print(f'f1       ={f1_score(labels_true, labels_predicted, average=\"macro\"):.3f}')\n","    return ;"]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":25,"status":"ok","timestamp":1678565229157,"user":{"displayName":"Ryosuke HARA","userId":"14758738608613309303"},"user_tz":0},"id":"xdtrzcTKP42h","outputId":"305d07b7-8feb-4aa0-f826-a663b0d8b743"},"outputs":[{"name":"stdout","output_type":"stream","text":["accuracy =0.143\n","precision=0.030\n","recall   =0.164\n","f1       =0.051\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]}],"source":["labels_predicted = predictions.argmax(axis=1)\n","showPerformance(label_ids, labels_predicted)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Q5 (c) Parameter Tuning is conducted in a file \"q5_param.ipynb\""]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Q6 Evaluation with TestData"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# ここから下はゴミ"]},{"cell_type":"markdown","metadata":{"id":"JARKa4a6bjR5"},"source":["## Tune Parameters"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":22,"status":"ok","timestamp":1678565482495,"user":{"displayName":"Ryosuke HARA","userId":"14758738608613309303"},"user_tz":0},"id":"96lqTAwdkuUW"},"outputs":[],"source":["# 3 sets of parameters\n","# Batch size cannot be over 16 due to memory issue.\n","\n","# make model sensitive to each data by decreasing batch_size and increasing learning_rate\n","param_a = {\"learning_rate\":1e-3,\n","           \"epochs\": 10,\n","           \"batch_size\":4}\n","\n","# make model less sensitive to each data by increasing batch_size and decreasing learning_rate. increase epochs instead to help converge.\n","param_b = {\"learning_rate\":1e-5,\n","           \"epochs\":50 ,\n","           \"batch_size\":16}\n","\n","# middle of A and B\n","param_c = {\"learning_rate\":1e-4,\n","           \"epochs\": 25,\n","           \"batch_size\":8}\n","params = [param_a,param_b, param_c]\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"112JO45sgoAy"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":22,"status":"ok","timestamp":1678565482495,"user":{"displayName":"Ryosuke HARA","userId":"14758738608613309303"},"user_tz":0},"id":"opbUGlR6nRTl"},"outputs":[],"source":["from transformers import TrainingArguments, Trainer,DataCollatorWithPadding\n","\n","def tuneParams(p,name):\n","  # for idx,p in enumerate(params):\n","    # configuring parameters\n","  training_args = TrainingArguments(\n","      output_dir                 = f\"model_{name}\", # HuggingFace wants a name for your model\n","      evaluation_strategy        = \"epoch\", # How often we want to evaluate the model\n","      learning_rate              = p[\"learning_rate\"], # Hyperparameter\n","      per_device_train_batch_size= p[\"batch_size\"], # Hyperparameter\n","      per_device_eval_batch_size = p[\"batch_size\"], # Hyperparameter\n","      num_train_epochs           = p[\"epochs\"] # Hyperparameter\n","      # weight_decay=weight_decay, # Hyperparameter\n","  )\n","    \n","  data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n","    # training\n","  trainer = Trainer(\n","      model=model, # The model you want to train\n","      args=training_args, # The various training arguments set up above\n","      train_dataset=train_dataset, # The data to use to update the weights\n","      eval_dataset=validation_dataset, # The data to use \n","      tokenizer=tokenizer, # The tokenizer used on the data\n","      data_collator=data_collator, # A data collator that does clever things moving data around\n","      )\n","    \n","  trainer.train()\n","\n","  # measuring metrics with validation set\n","  predictions, label_ids, metrics = trainer.predict(validation_dataset)\n","  labels_predicted = predictions.argmax(axis=1)\n","  print(f'----------model{name}---------')\n","  showPerformance(label_ids, labels_predicted)\n","  print(\"\\n\")\n","  return ;\n","\n","# あとで消す\n","# Show classifier performance\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n","def showPerformance(labels_true, labels_predicted):\n","    print(f'accuracy ={accuracy_score(labels_true, labels_predicted):.3f}')\n","    print(f'precision={precision_score(labels_true, labels_predicted, average=\"macro\"):.3f}')\n","    print(f'recall   ={recall_score(labels_true, labels_predicted, average=\"macro\"):.3f}') \n","    print(f'f1       ={f1_score(labels_true, labels_predicted, average=\"macro\"):.3f}')\n","    return ;\n"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":22,"status":"ok","timestamp":1678565482496,"user":{"displayName":"Ryosuke HARA","userId":"14758738608613309303"},"user_tz":0},"id":"X-MQlmFe4jXK"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":22,"status":"ok","timestamp":1678565482496,"user":{"displayName":"Ryosuke HARA","userId":"14758738608613309303"},"user_tz":0},"id":"hpAYps_K4jPS"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u02dorK84jGw"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ET7m4pFJpJ8Q"},"outputs":[],"source":["tuneParams(params[0],'a')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VgUuENemwLK6"},"outputs":[],"source":["tuneParams(params[1],'b')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"lSut_M_RwOdO","outputId":"d6691bce-3061-4ddb-a5e2-39c5618fc035"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","***** Running training *****\n","  Num examples = 1070\n","  Num Epochs = 25\n","  Instantaneous batch size per device = 8\n","  Total train batch size (w. parallel, distributed & accumulation) = 8\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 3350\n","  Number of trainable parameters = 124653322\n","You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='3351' max='3350' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [3350/3350 54:22, Epoch 25/25]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>No log</td>\n","      <td>2.328768</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>No log</td>\n","      <td>2.305428</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>No log</td>\n","      <td>2.309634</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>2.306400</td>\n","      <td>2.309653</td>\n","    </tr>\n","    <tr>\n","      <td>5</td>\n","      <td>2.306400</td>\n","      <td>2.303613</td>\n","    </tr>\n","    <tr>\n","      <td>6</td>\n","      <td>2.306400</td>\n","      <td>2.304098</td>\n","    </tr>\n","    <tr>\n","      <td>7</td>\n","      <td>2.306400</td>\n","      <td>2.313816</td>\n","    </tr>\n","    <tr>\n","      <td>8</td>\n","      <td>2.298600</td>\n","      <td>2.315737</td>\n","    </tr>\n","    <tr>\n","      <td>9</td>\n","      <td>2.298600</td>\n","      <td>2.310245</td>\n","    </tr>\n","    <tr>\n","      <td>10</td>\n","      <td>2.298600</td>\n","      <td>2.305802</td>\n","    </tr>\n","    <tr>\n","      <td>11</td>\n","      <td>2.298600</td>\n","      <td>2.298477</td>\n","    </tr>\n","    <tr>\n","      <td>12</td>\n","      <td>2.293900</td>\n","      <td>2.299958</td>\n","    </tr>\n","    <tr>\n","      <td>13</td>\n","      <td>2.293900</td>\n","      <td>2.308859</td>\n","    </tr>\n","    <tr>\n","      <td>14</td>\n","      <td>2.293900</td>\n","      <td>2.312730</td>\n","    </tr>\n","    <tr>\n","      <td>15</td>\n","      <td>2.291200</td>\n","      <td>2.304830</td>\n","    </tr>\n","    <tr>\n","      <td>16</td>\n","      <td>2.291200</td>\n","      <td>2.302495</td>\n","    </tr>\n","    <tr>\n","      <td>17</td>\n","      <td>2.291200</td>\n","      <td>2.302500</td>\n","    </tr>\n","    <tr>\n","      <td>18</td>\n","      <td>2.291200</td>\n","      <td>2.306797</td>\n","    </tr>\n","    <tr>\n","      <td>19</td>\n","      <td>2.287100</td>\n","      <td>2.301455</td>\n","    </tr>\n","    <tr>\n","      <td>20</td>\n","      <td>2.287100</td>\n","      <td>2.302620</td>\n","    </tr>\n","    <tr>\n","      <td>21</td>\n","      <td>2.287100</td>\n","      <td>2.305723</td>\n","    </tr>\n","    <tr>\n","      <td>22</td>\n","      <td>2.287100</td>\n","      <td>2.304253</td>\n","    </tr>\n","    <tr>\n","      <td>23</td>\n","      <td>2.289700</td>\n","      <td>2.302897</td>\n","    </tr>\n","    <tr>\n","      <td>24</td>\n","      <td>2.289700</td>\n","      <td>2.304124</td>\n","    </tr>\n","  </tbody>\n","</table><p>\n","    <div>\n","      \n","      <progress value='25' max='45' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [25/45 00:06 < 00:05, 3.62 it/s]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["***** Running Evaluation *****\n","  Num examples = 357\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 357\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 357\n","  Batch size = 8\n","Saving model checkpoint to model_c/checkpoint-500\n","Configuration saved in model_c/checkpoint-500/config.json\n","Model weights saved in model_c/checkpoint-500/pytorch_model.bin\n","tokenizer config file saved in model_c/checkpoint-500/tokenizer_config.json\n","Special tokens file saved in model_c/checkpoint-500/special_tokens_map.json\n","***** Running Evaluation *****\n","  Num examples = 357\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 357\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 357\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 357\n","  Batch size = 8\n","Saving model checkpoint to model_c/checkpoint-1000\n","Configuration saved in model_c/checkpoint-1000/config.json\n","Model weights saved in model_c/checkpoint-1000/pytorch_model.bin\n","tokenizer config file saved in model_c/checkpoint-1000/tokenizer_config.json\n","Special tokens file saved in model_c/checkpoint-1000/special_tokens_map.json\n","***** Running Evaluation *****\n","  Num examples = 357\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 357\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 357\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 357\n","  Batch size = 8\n","Saving model checkpoint to model_c/checkpoint-1500\n","Configuration saved in model_c/checkpoint-1500/config.json\n","Model weights saved in model_c/checkpoint-1500/pytorch_model.bin\n","tokenizer config file saved in model_c/checkpoint-1500/tokenizer_config.json\n","Special tokens file saved in model_c/checkpoint-1500/special_tokens_map.json\n","***** Running Evaluation *****\n","  Num examples = 357\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 357\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 357\n","  Batch size = 8\n","Saving model checkpoint to model_c/checkpoint-2000\n","Configuration saved in model_c/checkpoint-2000/config.json\n","Model weights saved in model_c/checkpoint-2000/pytorch_model.bin\n","tokenizer config file saved in model_c/checkpoint-2000/tokenizer_config.json\n","Special tokens file saved in model_c/checkpoint-2000/special_tokens_map.json\n","***** Running Evaluation *****\n","  Num examples = 357\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 357\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 357\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 357\n","  Batch size = 8\n","Saving model checkpoint to model_c/checkpoint-2500\n","Configuration saved in model_c/checkpoint-2500/config.json\n","Model weights saved in model_c/checkpoint-2500/pytorch_model.bin\n","tokenizer config file saved in model_c/checkpoint-2500/tokenizer_config.json\n","Special tokens file saved in model_c/checkpoint-2500/special_tokens_map.json\n","***** Running Evaluation *****\n","  Num examples = 357\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 357\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 357\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 357\n","  Batch size = 8\n","Saving model checkpoint to model_c/checkpoint-3000\n","Configuration saved in model_c/checkpoint-3000/config.json\n","Model weights saved in model_c/checkpoint-3000/pytorch_model.bin\n","tokenizer config file saved in model_c/checkpoint-3000/tokenizer_config.json\n","Special tokens file saved in model_c/checkpoint-3000/special_tokens_map.json\n","***** Running Evaluation *****\n","  Num examples = 357\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 357\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 357\n","  Batch size = 8\n"]}],"source":["tuneParams(params[2],'c')"]},{"cell_type":"markdown","metadata":{"id":"Lf5cHw94pKcc"},"source":["### rubbish\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a8YIIRkJpUT-"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RP4slqcYpVfx"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"w5qM7Ib_pWFk"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dgtmT1nWpWWw"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QOS6YFM8pWeK"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wDZ7vDecpWkB"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6in-FSo5pWn7"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qsn6ZZSLpWrh"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":1213309,"status":"ok","timestamp":1678553468458,"user":{"displayName":"Ryosuke HARA","userId":"14758738608613309303"},"user_tz":0},"id":"t_s4aqYDgYKI","outputId":"d1e0815b-c011-4f49-c87b-892402705238"},"outputs":[{"name":"stderr","output_type":"stream","text":["***** Running training *****\n","  Num examples = 1070\n","  Num Epochs = 10\n","  Instantaneous batch size per device = 16\n","  Total train batch size (w. parallel, distributed & accumulation) = 16\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 670\n","  Number of trainable parameters = 124653322\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='670' max='670' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [670/670 19:59, Epoch 10/10]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>No log</td>\n","      <td>2.120549</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>No log</td>\n","      <td>2.319053</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>No log</td>\n","      <td>2.318110</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>No log</td>\n","      <td>2.307925</td>\n","    </tr>\n","    <tr>\n","      <td>5</td>\n","      <td>No log</td>\n","      <td>2.301143</td>\n","    </tr>\n","    <tr>\n","      <td>6</td>\n","      <td>No log</td>\n","      <td>2.288367</td>\n","    </tr>\n","    <tr>\n","      <td>7</td>\n","      <td>No log</td>\n","      <td>2.296800</td>\n","    </tr>\n","    <tr>\n","      <td>8</td>\n","      <td>2.252600</td>\n","      <td>2.318459</td>\n","    </tr>\n","    <tr>\n","      <td>9</td>\n","      <td>2.252600</td>\n","      <td>2.301956</td>\n","    </tr>\n","    <tr>\n","      <td>10</td>\n","      <td>2.252600</td>\n","      <td>2.301772</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["***** Running Evaluation *****\n","  Num examples = 357\n","  Batch size = 16\n","***** Running Evaluation *****\n","  Num examples = 357\n","  Batch size = 16\n","***** Running Evaluation *****\n","  Num examples = 357\n","  Batch size = 16\n","***** Running Evaluation *****\n","  Num examples = 357\n","  Batch size = 16\n","***** Running Evaluation *****\n","  Num examples = 357\n","  Batch size = 16\n","***** Running Evaluation *****\n","  Num examples = 357\n","  Batch size = 16\n","***** Running Evaluation *****\n","  Num examples = 357\n","  Batch size = 16\n","Saving model checkpoint to singers_model/checkpoint-500\n","Configuration saved in singers_model/checkpoint-500/config.json\n","Model weights saved in singers_model/checkpoint-500/pytorch_model.bin\n","tokenizer config file saved in singers_model/checkpoint-500/tokenizer_config.json\n","Special tokens file saved in singers_model/checkpoint-500/special_tokens_map.json\n","***** Running Evaluation *****\n","  Num examples = 357\n","  Batch size = 16\n","***** Running Evaluation *****\n","  Num examples = 357\n","  Batch size = 16\n","***** Running Evaluation *****\n","  Num examples = 357\n","  Batch size = 16\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","***** Running Prediction *****\n","  Num examples = 357\n","  Batch size = 16\n"]},{"data":{"text/html":[],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}],"source":["\n","data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n","\n","\n","trainer = Trainer(\n","    model=model, # The model you want to train\n","    args=training_args, # The various training arguments set up above\n","    train_dataset=train_dataset, # The data to use to update the weights\n","    eval_dataset=validation_dataset, # The data to use \n","    tokenizer=tokenizer, # The tokenizer used on the data\n","    data_collator=data_collator, # A data collator that does clever things moving data around\n",")\n","trainer.train()\n","predictions, label_ids, metrics = trainer.predict(validation_dataset)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14,"status":"ok","timestamp":1678553468983,"user":{"displayName":"Ryosuke HARA","userId":"14758738608613309303"},"user_tz":0},"id":"i5Grz0eBgm7m","outputId":"a0487fc7-933d-441f-d8c2-3bec0b0800ea"},"outputs":[{"name":"stdout","output_type":"stream","text":["accuracy =0.084\n","precision=0.008\n","recall   =0.100\n","f1       =0.016\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]}],"source":["labels_predicted = predictions.argmax(axis=1)\n","showPerformance(label_ids, labels_predicted)"]},{"cell_type":"markdown","metadata":{"id":"BZIuPS7XuXuL"},"source":["# ここから下は適当に打ち込んでいる"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JMQgNiYSRk7G"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v1YWMtCMRoRw"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q8u2P0nyRotP"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xqKJWuj2Ro5D"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a-Dcvq7lRpBp"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IjONHIqORpJ_"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IRCZSim4RpRZ"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UH1ltG08RpaB"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uV1Q1qoURphl"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ib0-SVtcRpph"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5Tnya72BRpwd"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RGwxfhYQRp3R"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"q2zX88FqRp98"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"w6LVYubbRqEj"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VhwwPyK6mDL0"},"outputs":[],"source":["from transformers import AutoTokenizer\n","\n","tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")"]},{"cell_type":"markdown","metadata":{"id":"D1MRXcsVmVX3"},"source":["#### Create dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8fWu0aXLmOCy"},"outputs":[],"source":["# correspond labels(songer names) with integers\n","label2id = {label: i for i, label in enumerate(labels.)}"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":663,"status":"ok","timestamp":1677670223124,"user":{"displayName":"Ryosuke HARA","userId":"14758738608613309303"},"user_tz":0},"id":"y7dHUFrApPer","outputId":"260051fe-5d10-4add-829d-d43017441365"},"outputs":[{"name":"stderr","output_type":"stream","text":["loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--roberta-base/snapshots/ff46155979338ff8063cdad90908b498ab91b181/config.json\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"roberta-base\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.26.1\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--roberta-base/snapshots/ff46155979338ff8063cdad90908b498ab91b181/pytorch_model.bin\n","Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.weight', 'roberta.pooler.dense.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'roberta.pooler.dense.weight']\n","- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["from transformers import set_seed, AutoModelForSequenceClassification\n","\n","set_seed(42)\n","\n","model = AutoModelForSequenceClassification.from_pretrained('roberta-base')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UVHVnLw_jbsn"},"outputs":[],"source":["# parameters\n","learning_rate = 1e-4\n","epochs = 1\n","batch_size = 16\n","weight_decay = None\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":27,"status":"ok","timestamp":1677672718906,"user":{"displayName":"Ryosuke HARA","userId":"14758738608613309303"},"user_tz":0},"id":"nGFYLYT8mvnz","outputId":"6ae79574-0100-45e6-889f-5ddfa92cc36d"},"outputs":[{"data":{"text/plain":["{'input_ids': \"Perfect Illusion\\nPerfect Illusion Lyrics\\nTryin' to get control\\nPressure's takin' its toll\\nStuck in the middle zone\\nI just want you alone\\nMy guessing game is strong\\nWay too real to be wrong\\nCaught up in your show\\nYeah, at least now I know\\n\\nIt wasn't love, it wasn't love\\nIt was a perfect illusion (Perfect illusion)\\nMistaken for love, it wasn't love\\nIt was a perfect illusion (Perfect illusion)\\nYou were a perfect illusion\\nI don't need eyes to see\\nI felt you touchin' me\\nHigh like amphetamine\\nMaybe you're just a dream\\nThat's what it means to crush\\nNow that I'm wakin' up\\nI still feel the blow\\nBut at least now I know\\n\\nIt wasn't love, it wasn't love\\nIt was a perfect illusion (Perfect illusion)\\nMistaken for love, it wasn't love\\nIt was a perfect illusion (Perfect illusion)\\n\\nWhere are you?\\n'Cause I can't see you\\nIt was a perfect illusion\\nBut I feel you watchin' me\\nDilated, falling free\\nIn a modern ecstasy\\nWhere are you?\\n'Cause I can't see you\\nIt was a perfect illusion\\nBut I feel you watchin' me\\nBut I feel you watchin' me\\nIllusion\\nBut I feel you watchin' me\\nMistaken for love\\nWhere were you\\n'Cause I can't see\\nBut I feel you watchin' me\\nMistaken for love\\nDilated, falling free\\nIn a modern ecstasy\\nMistaken for love\\nIn a modern ecstasy\\nIn a modern ecstasy\\nI'm over the show\\nYeah, at least now I know\\nYou might also like\\nIt wasn't love, it wasn't love\\nIt was a perfect illusion (Perfect illusion)\\nMistaken for love, it wasn't love\\nIt was a perfect illusion (Perfect illusion)\\nOh-oh-oh, you were a perfect illusion\\nOh-oh-oh, it was a perfect illusion\\nIt was a perfect illusion\\nSomewhere in all the confusion\\nIt was a perfect illusion, illusion, illusion\\n\\nWhere were you 'cause I can't see it\\nIt was perfect illusion\\nBut I feel you watchin' me, baby\\nSomewhere in all the confusion\\nDilated, fallin' free\\nYou were so perfect\\nIn a modern ecstasy\\nYou were a, you were a perfect illusion88Embed\",\n"," 'labels': 'Lady Gaga'}"]},"execution_count":115,"metadata":{},"output_type":"execute_result"}],"source":["from datasets import Dataset\n","import numpy as np\n","\n","# labels_train = np.random.randint(0,9,1070)\n","# labels_val = np.random.randint(0,9,357)\n","\n","train_dataset = Dataset.from_dict({\n","    \"input_ids\":np.array(texts_train),\n","    \"labels\":np.array(labels_train)})\n","validation_dataset = Dataset.from_dict({\n","    \"input_ids\":np.array(texts_val), \n","    \"labels\":np.array(labels_val)})\n","train_dataset[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":24,"status":"ok","timestamp":1677672718906,"user":{"displayName":"Ryosuke HARA","userId":"14758738608613309303"},"user_tz":0},"id":"_dsvdIksmQ0q","outputId":"8ea587ef-68d3-470c-a8e4-26a8b552c25b"},"outputs":[{"name":"stderr","output_type":"stream","text":["PyTorch: setting up devices\n","The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"]}],"source":["from transformers import TrainingArguments, Trainer\n","\n","training_args = TrainingArguments(\n","    output_dir=\"singers_model\", # HuggingFace wants a name for your model\n","    evaluation_strategy=\"epoch\", # How often we want to evaluate the model\n","    learning_rate=learning_rate, # Hyperparameter\n","    per_device_train_batch_size=batch_size, # Hyperparameter\n","    per_device_eval_batch_size=batch_size, # Hyperparameter\n","    num_train_epochs=epochs, # Hyperparameter\n","    weight_decay=weight_decay, # Hyperparameter\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_dVJ6QMzozdp"},"outputs":[],"source":["#from transformers import DataCollatorForTokenClassification\n","\n","#data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n","from transformers import DataCollatorWithPadding\n","\n","data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n","\n","\n","trainer = Trainer(\n","    model=model, # The model you want to train\n","    args=training_args, # The various training arguments set up above\n","    train_dataset=train_dataset, # The data to use to update the weights\n","    eval_dataset=validation_dataset, # The data to use \n","    tokenizer=tokenizer, # The tokenizer used on the data\n","    data_collator=data_collator, # A data collator that does clever things moving data around\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"elapsed":17,"status":"ok","timestamp":1677672718907,"user":{"displayName":"Ryosuke HARA","userId":"14758738608613309303"},"user_tz":0},"id":"JxrVQUEgvg-u","outputId":"9c9fa1bc-7c73-4343-dd9b-975406706400"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'Why Don’t You Love Me\\nWhy Don’t You Love Me Lyrics\\nNow, now, now, honey\\nYou better sit down and look'"]},"execution_count":118,"metadata":{},"output_type":"execute_result"}],"source":["train_dataset[18][\"input_ids\"][:100]\n","# for t in train_dataset:\n","  # print(type(t[\"input_ids\"]))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":16,"status":"error","timestamp":1677672718908,"user":{"displayName":"Ryosuke HARA","userId":"14758738608613309303"},"user_tz":0},"id":"Ie1YRb9yo5GR","outputId":"1e4fdbf5-ede6-4d42-f32c-b674806e50e1"},"outputs":[{"name":"stderr","output_type":"stream","text":["***** Running training *****\n","  Num examples = 1070\n","  Num Epochs = 1\n","  Instantaneous batch size per device = 16\n","  Total train batch size (w. parallel, distributed & accumulation) = 16\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 67\n","  Number of trainable parameters = 124647170\n"]},{"ename":"ValueError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-119-3435b262f1ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1541\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inner_training_loop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_batch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_find_batch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1542\u001b[0m         )\n\u001b[0;32m-> 1543\u001b[0;31m         return inner_training_loop(\n\u001b[0m\u001b[1;32m   1544\u001b[0m             \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1545\u001b[0m             \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1763\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1764\u001b[0m             \u001b[0mstep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1765\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_iterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1766\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1767\u001b[0m                 \u001b[0;31m# Skip past any already trained steps if resuming training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    626\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 628\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    629\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    669\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 671\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    672\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/data/data_collator.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, features)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 249\u001b[0;31m         batch = self.tokenizer.pad(\n\u001b[0m\u001b[1;32m    250\u001b[0m             \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m             \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mpad\u001b[0;34m(self, encoded_inputs, padding, max_length, pad_to_multiple_of, return_attention_mask, return_tensors, verbose)\u001b[0m\n\u001b[1;32m   2969\u001b[0m                 \u001b[0mreturn_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"np\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreturn_tensors\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2970\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2971\u001b[0;31m                 raise ValueError(\n\u001b[0m\u001b[1;32m   2972\u001b[0m                     \u001b[0;34mf\"type of {first_element} unknown: {type(first_element)}. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2973\u001b[0m                     \u001b[0;34m\"Should be one of a python, numpy, pytorch or tensorflow object.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: type of Postcard (New Shoes)\nPostcard (New Shoes) Lyrics\nHey, honey\nCat got ya tongue?\nDon't look so funny\nMr. Comedian, joke's on you and\nYou good at bets, so where's my money?\nI know you bet I'd never make that move\nSo, why's the house empty?\nLights off by the end of this week\nI leave wit' what I came wit', baby\nI'll take the dogs, you keep the Mercedes\n'Cause I'm gone, I'm not fazed\nI walk these roads with grace\nAnd I'll wave with you behind me\nWith a new smile on my face, 'cause\n\nI'm walkin' in new shoes now\nI got a new song to sing\nWhen I walk in the room, every head turns\nEvery eye is on me\nToo bad you're not here to see it\nAnd by the time you get this\nI'll be so long, gone and far\nI'll send you a postcard\nI'll send you a postcard\n\nHey, baby\nYou seen a ghost?\nThis ain't yo' lady\nThis is the chick that you held back\nNow, all of the things you said were crazy\nI checked off my list and I feel good about it\nI got the new haircut\nThe one that you said wouldn't look no good\nTook that job you said I could not get\nWore that dress you said I'd never fit\nYou might also like\n'Cause you're gone, and I'm great\nI got rid of your dead weight\nAnd I'll wave at you behind me\nWith a new smile on my face, 'cause\n\nI'm walkin' in new shoes now\nI got a new song to sing\nWhen I walk in the room, every head turns\nEvery eye is on me\nToo bad you're not here to see it\nAnd by the time you get this\nI'll be so long, gone and far\nI'll send you a postcard\nI'll send you a postcardEmbed unknown: <class 'str'>. Should be one of a python, numpy, pytorch or tensorflow object."]}],"source":["trainer.train()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SpbMzFruo-c7"},"outputs":[],"source":["predictions, label_ids, metrics = trainer.predict(validation_dataset)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":166},"executionInfo":{"elapsed":1423,"status":"error","timestamp":1677666785705,"user":{"displayName":"Ryosuke HARA","userId":"14758738608613309303"},"user_tz":0},"id":"KNXQ3nzbo_FE","outputId":"f2f30102-4fbf-458d-a97d-079b951dcd3d"},"outputs":[{"ename":"NameError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-18-71cacc06bb91>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmetrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'metrics' is not defined"]}],"source":["metrics"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mQAjYVyBpAjp"},"outputs":[],"source":["print(f\"{type(predictions)=}\")\n","print(f\"{predictions.shape=}\")"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyMLsOjlbwhyoJiTBM+HYXoG","provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"}},"nbformat":4,"nbformat_minor":0}
