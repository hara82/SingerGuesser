\documentclass[a4paper,11pt]{article}


% 数式
\usepackage{amsmath,amsfonts}
\usepackage{bm}
\usepackage{here}
% 画像
\usepackage[dvipdfmx]{graphicx}
\usepackage[dvipdfmx]{color}
% hyperlink
\usepackage{hyperref}

\usepackage{multirow}
\usepackage{colortbl}


\begin{document}

\title{Text-as-Data Learning Coursework}
\author{2822260H Ryosuke Hara}
\date{\today}
\maketitle

\section{Q1- Dataset}
\subsection{a) Overview of data}% 数式
The dataset used in this experiment is the lyrics of songs of famous singers.
Singers are chosen from a ranking in a website, \href{https://www.thefamouspeople.com/21st-century-singers.php}{"The greatest 21st century singers"}, to minimise the arbitrary choice of the author.

Lyrics were obtained by using lyricsgenius, a python API of Genius.com.
Since lyrics were fetched online, it includes some HTML tokens and some meta data about translation inside the lyrics.
The meta data about translation are supposed to be removed by preprocessing.
Also, there are some faulty data where texts are not lyrics or lyrics are too short because it is cut in the middle of a song.
Too short lyrics where the total length is under 300 characters are removed automatically, and too long data are removed manually by inspecting the text.
To minimise the duplication of a same song, songs including certain keywords,such as "remix", "vergion", are ignored.


The main objective is to infer the singer from lyrics of a song.
I chose this dataset because I wanted to examine whether lyrics reflect singers' characteristics like the musical elements do.

Automatic classification has several applications.
It can be used for helping singers maintain the same trend among an album,
suggesting new singers for music listeners in recommendation system of music service like Spotify, 
detecting improper mimicking of the lyrics among singers. 

\subsection{b) Overview of input texts and labels}
summary of the labels and input text to be used \\

There are ten labels and each label is a name of a singer, which is to be predicted.
Singers are: "Ariana Grande", "Michael Jackson","Taylor Swift","XXXTentacion","Eminem", "Lady Gaga","Selena Gomez","Beyonce Knowles","Dua Lipa",'Jennifer Lopez.'
This dataset is not multi-label and does not consider a song from more than 2 singers.
Preprocessing of labels were unnecessary because texts were collected from a label.


Input texts are a title of a song and its lyrics concatenated with a new line character.
There are 1784 documents in total, 
and the length of input texts varies from 305 characters to 13102 characters.
The distribution of the length of text is shown in Figure 1.
\begin{figure}[htbp]
  \begin{center}
  \includegraphics[width=70mm]{figures/text_length.png}
  \caption{distribution of text length}
  \end{center}
\end{figure}



\subsection{c) spliting data}
The dataset is not split into a training, validation, and test set at first, 
so it was split into 60/20/20\% using "train\_test\_split" of scikit-learn.
The label counts for each split dataset is shown in Figure 2.
All labels have 100 to 200 documents in total and are not too imbalanced.



\begin{figure}[htbp]
  \begin{center}
  \includegraphics[width=140mm]{figures/text_labels2.png}
  \caption{label counts for each split}
  \end{center}
\end{figure}

% \begin{table}[htbp]
%   \centering
%   \begin{tabular}{l|c|r}
%     1 & 2 & 3 \\ \hline\hline
%     sin & cos & tan \\
%     apple & pine & banana \\ \hline
%   \end{tabular}
%   \caption{fugafuga}
%   \label{tb:fugafuga}
% \end{table}


% \subsection{steps to build the dataset}

\section{Q2- Clustering}
\subsection{Vectorizing texts}
Firstly, texts were converted into vectors to conduct clustering.
TfidfVectorizer from scikit-learn, along with 'en\_core\_web\_sm' from spacy were used.

Before converted to vectors, texts were firstly tokenized, and also stop words were removed.
Stop words were defined by the spacy models and manually added words, which are mainly interjections as "oh", "ah."
Such manually added words were chosen so that they do not show in the top 5 tokens of each cluster.
Stop words were removed because they do not convey much meaning and prevent understanding the characteristics of each cluster.

As for input, a title and lyrics of a song combined was used for an input because a title tend to be too short.


\subsection{a) Documents assigned to each cluster}
K-mean clustering was conducted with k=5.
The first 120 characters of 2 documents in each cluster are shown below.
Newline characters are sometimes substituted with period(.) to shorten the table.

\begin{table}[htb]

\begin{center}
\caption{2 documents in each cluster}

\small
\begin{tabular}{|c|c|} \hline

  Label & Text \\ \hline \hline

    0&
    \begin{tabular}{l}
      
    Moonlight. 
    Lyrics. \\
    The sun is setting and you're right here by my side \\
    And the movie is playing, but we won't be watching \\
    \end{tabular}\\ \hline

    0&
    \begin{tabular}{l}
      
    Shady XV.
    Shady XV Lyrics.\\
    I'm liable to start a violent spark with a silent thought \\
    I disgust you like dialogue from The  \\ 
    \end{tabular}\\ \hline

    1&
    \begin{tabular}{l}
  Don't Walk Away. 
  Don't Walk Away Lyrics. \\
  Ooh, don't walk away.
  Walk away.
  Don't walk away.\\
  See, I just can't find the right  \\
    \end{tabular}\\ \hline

    1&
    \begin{tabular}{l}
  On the Floor (Mixin Marc \& Tony Svejda L.A. to Ibiza Mix) \\
  On the Floor (Mixin Marc \& Tony Svejda L.A. to Ibiza Mix) Lyri 
    \end{tabular}\\ \hline
  
    2&
    \begin{tabular}{l}
    INUYASHA.
    Lyrics.\\
    Damn, baby, what's your fucking name? god damn (Murder)\\
    Goddamn, goddamn.
    100 hoes.
    BitBoy Beats.
    Acid, aci
    \end{tabular}\\ \hline


    2&
    \begin{tabular}{l}
    rare.
    Lyrics.\\
    Very fucking holy, very fucking rare \\
    Even in broad daylight, I have this dream, I have this fantasy \\
    Marvelo
    \end{tabular}\\ \hline


    3&
    \begin{tabular}{l}
Perfect Illusion.
Perfect Illusion Lyrics.\\
Tryin' to get control.
Pressure's takin' its toll \\
Stuck in the middle zone.
I jus
    \end{tabular}\\ \hline


    3&
    \begin{tabular}{l}
Freakshow.
Freakshow Lyrics.\\
'Cause I guarantee you've never seen a show like this before\\
Gonna show you something that yo
    \end{tabular}\\ \hline

    
    4&
    \begin{tabular}{l}
Scheiße (DJ White Shadow Mugler)\\
Scheiße (DJ White Shadow Mugler) Lyrics\\
I don't speak German, but I can if you like, ow
    \end{tabular}\\ \hline
    4&
    \begin{tabular}{l}
The Manifesto of Chromatica\\
The Manifesto of Chromatica Lyrics\\
When I was younger, I was at some point, born.
I grew in 
    \end{tabular}\\ \hline
    
    
    
    
  \end{tabular}

\end{center}

\end{table}

 
Besides, the top 5 tokens with highest magnitude in each centroid is listed below.  


\begin{table}[htb]

  \begin{center}
  \caption{top 5 tokens in each centroid}

  \begin{tabular}{|c|c|} 
    label & tokens\\ \hline \hline
    0 & know, feel, like, fuck, let \\ \hline
    1 & stay, bitch, away, work, walk\\ \hline
    2 & animal, jump, damn, glad, effect\\ \hline
    3 & want, know, love, like, baby\\ \hline
    4 & like, people, world, woman, million\\ \hline
  \end{tabular}
  \end{center}
\end{table}


\subsection{b) Examining clusters}
There are no easily recognisable distinct characteristics in each cluster shown in Table 1 and 2.
For example in Table 2, the token 'like' appeared in 3 clusters, and 'know' in 2.
As for topics, no significant difference among clusters.
\subsection{c) Confusion matrix}
In Figure 3, a confusion matrix between 5 clusters and target labels is shown.

\begin{figure}[htbp]
  \begin{center}
  \includegraphics[width=100mm]{figures/confusion_mat.png}
  \caption{confusion matrix}
  \end{center}
\end{figure}
\subsection{d) Examining the confusion matrix}
From Figure 3, one can see that most texts are labelled to either cluster 0 or 3. 
For 'XXXTentacion' and 'Eminem', texts are gathered into 0 and a few texts into 3.
Texts of other singers are almost evenly separated into cluster 0 and 3.

Thus, non of the clusters are able to pick up a single target label.

% Subtle difference among clusters lies in the most prominent target label in a cluster.
% The most prominent label for each cluster is: cluster 0 for 'Eminem', 1 for 'Michael Jackson', 2 for 'XXXTentacion', 
% 3 for 'Michael Jackson', 4 for 'Ariana Grande'.
% However, 
The clusters differ subtly in terms of their most prominent target label. 
Specifically, the most prominent label for each cluster is as follows: Cluster 0 corresponds to 'Eminem', Cluster 1 corresponds to 'Michael Jackson', Cluster 2 corresponds to 'XXXTentacion', Cluster 3 corresponds to 'Michael Jackson', and Cluster 4 corresponds to 'Ariana Grande'.
This difference is, however, not significant at all.





\section{Q3- Comparing Classifiers}
\subsection{a) 5 baseline classifiers}
The five baseline classifiers indicated in the course specification were implemented.
The performance of them are shown in Table 3.
The highest performance is highlighted in yellow.

\begin{table}[htbp]
    \centering
    \caption{Comparison of Metrics for 5 Classifiers}
    \label{tab:classifier_metrics}
    \small
    \scalebox{0.8}[0.8]{
    \begin{tabular}{l|cccc|cccc}
        \hline
        \textbf{Classifier} & \multicolumn{4}{c|}{\textbf{Validation Metrics}} & \multicolumn{4}{c}{\textbf{Training Metrics}} \\
        & \textbf{Accuracy} & \textbf{Precision} & \textbf{Recall} & \textbf{F1} & \textbf{Accuracy} & \textbf{Precision} & \textbf{Recall} & \textbf{F1} \\
        \hline
        Dummy Classifier (most\_frequent) & {0.084} & 0.008 & 0.100 & 0.016 & 0.127 & 0.013 & 0.100 & 0.023 \\
        Dummy Classifier (stratified) & 0.106 & 0.101 & 0.101 & 0.100 & 0.095 & 0.090 & 0.090 & 0.090 \\
        \rowcolor[rgb]{0.9,0.9,0}Logistic Regression (one-hot) & \colorbox[rgb]{0.9, 0.9, 0}{0.563} & \colorbox[rgb]{0.9, 0.9, 0}{0.565} & \colorbox[rgb]{0.9, 0.9, 0}{0.567} & \colorbox[rgb]{0.9, 0.9, 0}{0.556} & 1.000 & 1.000 & 1.000 & 1.000 \\
        Logistic Regression (TF-IDF) & 0.451 & 0.424 & 0.444 & 0.413 & 0.895 & 0.906 & 0.867 & 0.874 \\
        SVC (one-hot) & 0.518 & 0.498 & 0.514 & 0.481 & 0.936 & 0.943 & 0.912 & 0.919 \\
        \hline
    \end{tabular}
    }
\end{table}

As shown in Table 3, the best-performing classifier by macro F1 on validation set is Logistic Regression with One-hot vectorization.
A bar chart graph for F1 score for each class by this classifier is shown in Figure 4.
\begin{figure}[htbp]
  \begin{center}
  \includegraphics[width=100mm]{figures/F1score.png}
  \caption{F1 Score for Each Class by Logistic Regression(One-hot)}
  \end{center}
\end{figure}

The performance of dummy classifiers were low as F1 score were below 0.100,
whereas the F1 scores of the other 3 classifiers were above 0.4 on validation set,
and above 0.85 on training set.
This shows non-dummy classifiers overfit to Training set because of the gap between training and validation set.

Since each labels are almost equally distributed, the performance difference among labels can be attributed to the characteristics of texts.
Logistic Regression(One-hot) classifiers worked well on 'XXXTentacion', 'Eminem', and 'Taylor Swift'.
The low F1 score of 'Selena Gomez' might result from the fewer number of texts labelled.

Apart from determining the random state and augmenting maximum iteration, the default parameters of scikit-learn classifiers were used.


\subsection{b) My chosen classifier}


\section{Q4- Parameter Tuning}
\subsection{Classifier}
\subsection{Vectorizer}
\subsection{My own parameter}


\section{Context vectors using BERT}
\subsection{feature extraction and logistic regression}
\subsection{end-to-end trained classifier}
\subsection{Selecting hyperparameters and models}
\subsection{Examining models and parameters}


\section{Q6- Conclusions and Future Work}
















\end{document}