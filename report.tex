\documentclass[a4paper,11pt]{article}
%余白を変更
\usepackage[margin=18mm]{geometry}


% 数式
\usepackage{amsmath,amsfonts}
\usepackage{bm}
\usepackage{here}
% 画像
\usepackage[dvipdfmx]{graphicx}
\usepackage[dvipdfmx]{color}
% hyperlink
\usepackage{hyperref}

\usepackage{multirow}
\usepackage{colortbl}


\title{\vspace{-10cm}trigonometric_function}
\begin{document}

\title{Text-as-Data Learning Coursework}
\author{2822260H Ryosuke Hara}
\date{\today}
\maketitle

\section{Q1- Dataset}
\subsection{a) Overview of Data}% 数式
The dataset used in this experiment is the lyrics of songs by famous singers. 
Singers are chosen from a ranking on a website called \href{https://www.thefamouspeople.com/21st-century-singers.php}{"The greatest 21st century singers,"}, to minimise the arbitrary choice of the author.
\small{\url{https://www.thefamouspeople.com/21st-century-singers.php}}

Lyrics were obtained by using lyricsgenius, a python API for Genius.com.
Since lyrics were fetched online, they include some HTML tokens and metadata about translation.
The metadata about translation is supposed to be removed by preprocessing.
Also, there is some faulty data where the texts are not lyrics or the lyrics are too short because they are cut in the middle of a song.
Lyrics with a total length of under 300 characters are automatically removed, and texts that are too long are removed manually after inspection.
To minimise the duplication of the same song, songs including certain keywords such as "remix" or "vergion" in their titles are ignored.


The main objective is to infer the singer from the lyrics of a song.
I chose this dataset because I wanted to examine whether lyrics reflect singers' characteristics like the musical elements do.

Automatic classification has several applications.
It can be used to help singers maintain the same trend across an album,
suggest new singers to music listeners in a recommendation system of a music service like Spotify, and
detect improper mimicking of the lyrics among other singers. 

\subsection{b) Overview of Input Texts and Labels}
% There are 10 labels and each label is a name of a singer, which is to be predicted.
% The singers are: "Ariana Grande", "Michael Jackson", "Taylor Swift", "XXXTentacion", "Eminem", "Lady Gaga", "Selena Gomez", "Beyonce Knowles", "Dua Lipa",
% "Jennifer Lopez".
% This dataset is not multi-label and does not consider a song from more than 2 singers.
% Preprocessing of labels was unnecessary because texts were collected from a label.


% Input texts are a title of a song and its lyrics concatenated with a new line character.
% There are 1784 documents in total, 
% and the length of input texts varies from 305 characters to 13102 characters.
% The distribution of the text length is shown in Figure 1.
% \begin{figure}[htbp]
%   \begin{center}
%   \includegraphics[width=70mm]{figures/text_length.png}
%   \caption{distribution of text length}
%   \end{center}
% \end{figure}
There are 10 labels, each representing the name of a singer, that need to be predicted.
The singers are shown in the first row of Figure 1.
% The singers are: "Ariana Grande," "Michael Jackson," "Taylor Swift," "XXXTentacion," "Eminem," "Lady Gaga," "Selena Gomez," "Beyonce Knowles," "Dua Lipa," and "Jennifer Lopez". 
This dataset is not multi-label and does not consider songs with more than two singers.
Preprocessing of labels was unnecessary since the data was collected from the labels themselves.

The input texts consist of the title of a song and its lyrics concatenated with a new line character.
There are a total of 1784 documents, and the length of the input texts varies from 305 characters to 13102 characters.



\subsection{c) Spliting Data}
The dataset was not split into training, validation, and test set at first, 
so it was split into 60/20/20\% using "train\_test\_split" of scikit-learn.
The label counts for each split dataset is shown in Figure 1.
All of the labels have between 100 and 200 documents in total, and the data is not too imbalanced.

\begin{figure}[htbp]
  \begin{center}
  \includegraphics[width=160mm]{figures/text_labels2.png}
  \caption{Label Counts for Each Split}
  \end{center}
\end{figure}

\section{Q2- Clustering}
\subsection{Vectorizing texts}
% Firstly, texts were converted into vectors to conduct clustering.
% TfidfVectorizer from scikit-learn, along with 'en\_core\_web\_sm' from spacy were used.
The texts were first converted into vectors using TfidfVectorizer from scikit-learn and "en\_core\_web\_sm" from spacy.


Before being converted to vectors, texts were firstly tokenized, and stop words were removed.
Stop words were defined by the spacy models and manually added words, which are interjections such as "oh" or "ah."
Such manually added words were chosen so that they would not appear in the top 5 tokens of each cluster.
Stop words were removed because they do not convey much meaning and prevent the understanding of the characteristics of each cluster.

% For the input, a title and lyrics of a song combined were used for the input because a title tends to be too short.
For the input, a combination of the title and lyrics of each song was used because the title alone tends to be too short but represents the song.

\subsection{a) Documents Assigned to Each Cluster}
K-means clustering was conducted with k=5.
The first 120 characters of one document from each cluster are shown in Table 1.
Newline characters are sometimes substituted with periods (.) to shorten the table.
Additionally, the title of another song within each label is also provided to show additional documents while still keeping the table concise.
%Also, the title of another song in a label is also provided to shorten the table but to show other documents.


\begin{table}[htb]
\begin{center}
\caption{Example of Documents in Each Cluster}

\small
\scalebox{0.8}[0.8]{
\begin{tabular}{|c|c|c|} \hline
  Label & Text & Other songs\\ \hline \hline

    0&
    \begin{tabular}{l}
    Moonlight. 
    Lyrics. \\
    The sun is setting and you're right here by my side \\
    And the movie is playing, but we won't be watching \\
    \end{tabular} 
    & Shady XV\\\hline

    % 0&
    % \begin{tabular}{l}
      
    % Shady XV.
    % Shady XV Lyrics.\\
    % I'm liable to start a violent spark with a silent thought \\
    % I disgust you like dialogue from The  \\ 
    % \end{tabular}\\ \hline

    1&
    \begin{tabular}{l}
  Don't Walk Away. 
  Don't Walk Away Lyrics. \\
  Ooh, don't walk away.
  Walk away.
  Don't walk away.\\
  See, I just can't find the right  \\
    \end{tabular}& On the Floor\\ \hline
  %   1&
  %   \begin{tabular}{l}
  % On the Floor (Mixin Marc \& Tony Svejda L.A. to Ibiza Mix) \\
  % On the Floor (Mixin Marc \& Tony Svejda L.A. to Ibiza Mix) Lyri 
  %   \end{tabular}\\ \hline
    2&
    \begin{tabular}{l}
    INUYASHA.
    Lyrics.\\
    Damn, baby, what's your fucking name? god damn (Murder)\\
    Goddamn, goddamn.
    100 hoes.
    BitBoy Beats.
    Acid, aci
    \end{tabular} & rare. by XXXTentacion\\ \hline
    % 2&
    % \begin{tabular}{l}
    % rare.(by)
    % Lyrics.\\
    % Very fucking holy, very fucking rare \\
    % Even in broad daylight, I have this dream, I have this fantasy \\
    % Marvelo
    % \end{tabular}\\ \hline


    3&
    \begin{tabular}{l}
Perfect Illusion.
Perfect Illusion Lyrics.\\
Tryin' to get control.
Pressure's takin' its toll \\
Stuck in the middle zone.
I jus
    \end{tabular} & Freakshow by Lady Gaga\\ \hline


%     3&
%     \begin{tabular}{l}
% Freakshow.
% Freakshow Lyrics.\\
% 'Cause I guarantee you've never seen a show like this before\\
% Gonna show you something that yo
%     \end{tabular}\\ \hline

    
    4&
    \begin{tabular}{l}
Scheiße (DJ White Shadow Mugler)\\
Scheiße (DJ White Shadow Mugler) Lyrics\\
I don't speak German, but I can if you like, ow
    \end{tabular} & The Manifesto of Chromatica\\ \hline
%     4&
%     \begin{tabular}{l}
% The Manifesto of Chromatica\\
% The Manifesto of Chromatica Lyrics\\
% When I was younger, I was at some point, born.
% I grew in 
% %     \end{tabular}\\ \hline
  \end{tabular}
}
\end{center}
\end{table}

 
Besides, the top 5 tokens with highest magnitude in each centroid are listed in Table 2.  


\begin{table}[htb]

  \begin{center}
  \caption{Top 5 tokens in Each Centroid}
  \small
  \begin{tabular}{|c|c|} 
    label & tokens\\ \hline \hline
    0 & know, feel, like, fuck, let \\ \hline
    1 & stay, bitch, away, work, walk\\ \hline
    2 & animal, jump, damn, glad, effect\\ \hline
    3 & want, know, love, like, baby\\ \hline
    4 & like, people, world, woman, million\\ \hline
  \end{tabular}
  \end{center}
\end{table}


\subsection{b) Examining Clusters}
There are no easily recognisable distinct characteristics in each cluster shown in Table 1 and 2.
For example in Table 2, the token 'like' appeared in 3 clusters, and 'know' in 2.
As for topics, there is no significant difference among clusters.
\subsection{c) Confusion Matrix}
In Figure 2, a confusion matrix between 5 clusters and target labels is shown.

\begin{figure}[htbp]
  \begin{center}
  \includegraphics[width=110mm]{figures/confusion_mat2.png}
  \caption{Confusion Matrix of Clustering}
  \end{center}
\end{figure}
\subsection{d) Examining the Confusion Matrix}
% From Figure 2, one can see that most texts are labelled to either cluster 0 or 3. 
% For 'XXXTentacion' and 'Eminem', texts are gathered into 0 and a few texts into 3.
% Texts of other singers are almost evenly separated into cluster 0 and 3.
% Thus, none of the clusters is able to pick up a single target label.

According to Figure 2, most of the texts are assigned to either cluster 0 or 3.
Specifically, texts from 'XXXTentacion' and 'Eminem' are mostly in cluster 0, while a few are in cluster 3.
Texts from the other singers are more evenly distributed between cluster 0 and 3.
As a result, none of the clusters are able to exclusively capture a single target label.

% The clusters differ subtly in terms of their most prominent target label. 
% Specifically, the most prominent label for each cluster is as follows: Cluster 0 corresponds to 'Eminem', Cluster 1 corresponds to 'Michael Jackson', Cluster 2 corresponds to 'XXXTentacion', Cluster 3 corresponds to 'Michael Jackson', and Cluster 4 corresponds to 'Ariana Grande'.
% This difference is, however, not significant at all.


\section{Q3- Comparing Classifiers}
\subsection{a) 5 Baseline Classifiers}
The five baseline classifiers indicated in the course specification were implemented.
The performance of them are shown in Table 3.
The highest performance is highlighted in yellow.

\begin{table}[htbp]
    \centering
    \caption{Comparison of Metrics for 5 Classifiers}
    \label{tab:classifier_metrics}
    % \small
    \scalebox{0.8}[0.8]{
    \begin{tabular}{l|cccc|cccc}
        \hline
        \textbf{Classifier} & \multicolumn{4}{c|}{\textbf{Validation Metrics}} & \multicolumn{4}{c}{\textbf{Training Metrics}} \\
        & \textbf{Accuracy} & \textbf{Precision} & \textbf{Recall} & \textbf{F1} & \textbf{Accuracy} & \textbf{Precision} & \textbf{Recall} & \textbf{F1} \\
        \hline
        Dummy Classifier (most\_frequent) & {0.084} & 0.008 & 0.100 & 0.016 & 0.127 & 0.013 & 0.100 & 0.023 \\
        Dummy Classifier (stratified) & 0.101 & 0.099 & 0.096 & 0.096 & 0.089 & 0.087 & 0.086 & 0.086 \\
        \rowcolor[rgb]{0.9,0.9,0}Logistic Regression (one-hot) & \colorbox[rgb]{0.9, 0.9, 0}{0.560} & \colorbox[rgb]{0.9, 0.9, 0}{0.563} & \colorbox[rgb]{0.9, 0.9, 0}{0.564} & \colorbox[rgb]{0.9, 0.9, 0}{0.554} & 1.000 & 1.000 & 1.000 & 1.000 \\
        Logistic Regression (TF-IDF) & 0.457 & 0.434 & 0.451 & 0.421 & 0.921 & 0.931 & 0.895 & 0.902 \\
        SVC (one-hot) & 0.499 & 0.493 & 0.496 & 0.466 & 0.927 & 0.940 & 0.902 & 0.910 \\
        \hline
    \end{tabular}
    }
\end{table}

As shown in Table 3, the best-performing classifier in terms of macro F1 on validation set is Logistic Regression with One-hot vectorization.
Figure 3 shows a bar chart graph of the F1 score for each class by this classifier.

\begin{figure}[htbp]
  \begin{center}
  \includegraphics[width=100mm]{figures/F1score.png}
  \caption{F1 Score for Each Class by Logistic Regression(One-hot)}
  \end{center}
\end{figure}

The performance of dummy classifiers was low with F1 score were below 0.100,
while the F1 scores of the other 3 classifiers were above 0.4 on the validation set
and above 0.85 on the training set.
This shows non-dummy classifiers overfit to Training set due to the gap between the training and validation sets.

Since each labels are almost equally distributed, the performance difference among labels can be attributed to the characteristics of the texts.
Logistic Regression(One-hot) classifiers worked well on 'XXXTentacion', 'Eminem', and 'Taylor Swift'.
The low F1 score of 'Selena Gomez' might be due to the fewer number of texts labelled.

Apart from determining the random state and augmenting maximum iteration, the default parameters of scikit-learn classifiers were used.


\subsection{b) My Chosen Classifier}
I chose a different preprocessing for the Tf-Idf vectorizer in a new classifier, where stop words and punctuation are not removed and Capital letters are not lowercased.
Stop words are not removed because the frequency and variation of such words might differ from one song to another, and simple words tend to be used in songs.
Punctuation in not removed because certain singers may use hyphenated words often, such as "P-p-p-poker" in "Poker Face" by Lady Gaga. 
Also, Capital letters can capture the words with which a phrase start, so they are not removed.

Logistic Regression was used to compare the result with the previous result.
The result is shown in Table 4.
It performed better than the 2 dummy clussifers, but worse than any of the 3 other classifiers in terms of all the metrics.
Therefore, it is concluded removing stopwords and punctuations and lowercasing are also effective in the case for lyrics.




\begin{table}[htbp]
    \centering
    \caption{Metrics for Classifier Without Stopwords Removal}
    % \small
    \scalebox{0.8}[0.8]{
    \begin{tabular}{l|cccc|cccc}
        \hline
        \textbf{Classifier} & \multicolumn{4}{c|}{\textbf{Validation Metrics}} & \multicolumn{4}{c}{\textbf{Training Metrics}} \\
        & \textbf{Accuracy} & \textbf{Precision} & \textbf{Recall} & \textbf{F1} & \textbf{Accuracy} & \textbf{Precision} & \textbf{Recall} & \textbf{F1} \\
        \hline 
        Logistic Regression &&&&&& \\ (TF-IDF with stop words) & 0.443 & 0.413 & 0.437 & 0.403 & 0.839 & 0.854 & 0.805 & 0.812 \\
        \hline
    \end{tabular}
    }
\end{table}






\section{Q4- Parameter Tuning}
\subsection{Classifier}
First, Regularisation C value are tuned, and the results are shown in Table 5.
It is shown $C=10$ is the best parameter.%, and the metrics gradually decreases as C increases beyond C=10.
% That means weak regularisation worked well. 

\begin{table}[htbp]
    \centering
    \caption{Performance with Different Regularisation C Value}
    % \small
    \scalebox{0.8}[0.8]{
    \begin{tabular}{l|cccc}
        C value & Accuracy & Precision & Recall & F1 \\ \hline 
        $10^{-3}$ &0.084 & 0.008& 0.100& 0.016\\ 
        $10^{-2}$ &0.084 &0.008 &0.100 &0.016\\ 
        $10^{-1}$ &0.263 &0.346 &0.269 &0.233\\ 
        $10^{0}$  &0.457 &0.434 &0.451 &0.421\\ 
        \rowcolor[rgb]{0.9,0.9,0}$10^{1}$  &0.485 &0.489 &0.484 &0.479\\ 
        $10^{2}$  &0.473 &0.469 &0.472 &0.466\\ 
        $10^{3}$  &0.468 &0.460 &0.467 &0.461\\ 
        $10^{4}$  &0.462 &0.455 &0.463 &0.456\\ 
        $10^{5}$  &0.462 &0.455 &0.462 &0.455\\ 

    \end{tabular}
    }
\end{table}
\subsection{Vectorizer}
The parameters of vectorizer,that are sublenear\_tf and max\_features, are tuned here.
The regularisation parameter C is set to 10 based on the previous results.
The best combination is max\_features=5000 and  sublinear\_tf = True (Table 6). 

\begin{table}[htbp]
    \centering
    \caption{Performance with Different Vectorizer Parameter}
    % \small
    \scalebox{0.8}[0.8]{
    \begin{tabular}{l|l|cccc}
        max\_features & sublinear\_tf & Accuracy & Precision & Recall & F1 \\ \hline 
        None&True&0.571&0.584&0.565&0.554 \\
        None&False&0.485&0.489&0.484&0.479\\
        5&True&0.196&0.124&0.203&0.147\\
        5&False&0.196&0.124&0.205&0.143\\
        50&True&0.412&0.415&0.423&0.407\\
        50&False&0.367&0.366&0.369&0.358\\
        500&True&0.499&0.504&0.501&0.495\\
        500&False&0.415&0.418&0.422&0.414\\
        \rowcolor[rgb]{0.9,0.9,0}5000&True&0.577&0.589&0.575&0.567\\
        5000&False&0.473&0.470&0.473&0.465\\
        50000&True&0.571&0.584&0.565&0.554\\
        50000&False&0.485&0.489&0.484&0.479\\
    \end{tabular}
    }
\end{table}
\subsection{Penalty Term}
% Penalty Term of the Logistic Regression was changed from 'none', 'l1','l2','elasticnet'.
% Since Table 3 suggests overfitting of a model, using different penalty term may result in more simple/complex models,
% and simpler models might work well on validation dataset. 
% Therefore, penalty term was tuned.
% To be able to use l1 penalty term, solver was changed from 'lbfgs' to 'saga'

The penalty term of the Logistic Regression was changed from 'none', 'l1', 'l2', to 'elasticnet' to tackle potential overfitting problems identified in Table 3.
By tuning the penalty term, I aim to create simpler or more complex models, with the expectation that simpler models might perform better on the validation dataset.
In order to use the l1 penalty term, solver was changed from 'lbfgs' to 'saga.'
The best performance was obtained by using l2 term as shown in Table 7.

\subsection{Interpretation of Parameter Choice}


In the case of the regularization C for Logistic Regression, a larger value tends to yield better results.
Thus, a model with more freedom was considered appropriate. 
For the Vectorizer, it was important to set the max features parameter to a sufficiently large value, 
as too few tokens cannot distinguish between separate documents.
Additionally, setting sublinear\_tf = True, which takes the logarithm of tf, led to better results.
This might be due to the characteristics of lyrics where same words and phrases are used repeatedly and frequently.

Regarding the Penalty term, using l2 penalty, which returns a more dense model than l1 penalty, resulted in better performance.
Combined with the discussion of C, a more complex model was found to yield better results.
One reason for this might be that the distinction between each document is subtle, as seen in Section 2 where most documents are classified into only two labels. 
Therefore, a complex model that is sensitive to tiny differences might be necessary.

\begin{table}[htbp]
    \centering
    \caption{Performance with Different Penalty Term}
    % \small
    \scalebox{0.8}[0.8]{
    \begin{tabular}{l|cccc}
        Penalty Term & Accuracy & Precision & Recall & F1 \\ \hline 
        none & 0.552 & 0.560 & 0.554 & 0.549 \\ 
        l1 & 0.499 & 0.496 & 0.503 & 0.492 \\
        \rowcolor[rgb]{0.9,0.9,0}l2 & 0.577 & 0.589 & 0.575 & 0.567 \\
        elasticnet(ratio=0.5) & 0.535 & 0.557 & 0.537 & 0.531 \\



    \end{tabular}

    }
\end{table}

\section{Context vectors using BERT}

\subsection{Feature Extraction and Logistic Regression}
Here, the texts are converted into context vectors using the 'feature-extraction' pipeline with the "roberta\_base" model in HuggingFace library.
The vectors of the first [CLS] tokens were used for a Logistic Regression classifier.
The results are shown in Table 8.

\begin{table}[htbp]
    \centering
    \caption{Metrics of Logistic Regression using [CLS] Token}
    % \small
    \scalebox{0.9}[0.9]{
    \begin{tabular}{l|cccc}
         Approach & Accuracy & Precision & Recall & F1 \\ \hline 
         Logistic Regression & 0.524 & 0.476 & 0.517 & 0.485 \\ 

    \end{tabular}

    }
\end{table}
\subsection{End-to-End Trained Classifier}
% An end-to-end classifier was implemented and evaluated using the indicated parameters at first.
% After that, hyper parameters were tuned and evaluated.
% The details of hyper parameters and the results are shown  in Table 9.

% parameter(a) was selected so that the model be sensitive to each data 
% by decreasing batch\_size and increasing learning rate.
% parameter(b) was selected so that the model be less sensitive to each data
% with large batch\_size and small learning\_rate. Epochs were increased to help model to converge.
% The maximum size of batch\_size was 16 due to memory problem.
% parameter(c) was chosen to be the middle of parameter(a) and parameter(b).

% Experiment showed parameter(b) yields higher scores than others on the validation set, and therefore
% parameter(d), parameter(e) and parameter(f), which are slightly modified from parameter(b) were chosen and tested.


% It turned out that parameter(e) yield the best results as in Table 9.
Initially, an end-to-end classifier was implemented and evaluated using the specified parameters.
After this, hyperparameters were tuned and evaluated, with details and results presented in Table 9.

Parameter (a) was chosen to increase the model's sensitivity to each data point by decreasing batch size and increasing learning rate,
while parameter (b) was chosen to make the model less sensitive to each data point using larger batch sizes and smaller learning rates.
Epochs were increased to help the model converge.
The batch size was set to 16 because this was the maximum due to memory constraints.
Parameter (c) was selected as a middle point between parameter (a) and parameter (b).

Experiments revealed that parameter (b) yielded higher scores than the other options on the validation set.
Therefore, parameter (d), parameter (e), and parameter (f), which were slightly modified from parameter (b), were selected and tested.

Ultimately, it was found that parameter (e) produced the best results, as shown in Table 9.

\begin{table}[htbp]
    \centering
    \caption{Metrics of End-to-End Classifier with Different Hyperparameters}
    \small
    \scalebox{0.8}[0.8]{
    \begin{tabular}{l|lll|cccc}
         Approach & Learning\_rate & Epochs & Batch\_size & Accuracy & Precision & Recall & F1 \\ \hline \hline
        %  Logistic Regression & 0.524 & 0.476 & 0.517 & 0.485 \\ 
         end-to-end Classifier (b) & 1e-4& 1 & 16 & 0.213 & 0.221 &0.207 & 0.118 \\ \hline
         parameter(a)                  & 1e-3& 10& 4  & 0.084 & 0.008 & 0.100 & 0.016 \\
        %  param(b)                  & 1e-5& 50& 16 & 0.594 & 0.598 & 0.595 & 0.586 \\
         parameter(b)                  & 1e-5& 50& 16 & 0.571 & 0.599 & 0.577 & 0.574 \\
         parameter(c)                  & 1e-4& 25& 8  & 0.084 & 0.008 & 0.100 & 0.016 \\ \hline
        %  param(d)                  & 1e-5& 50& 8  & 0.605 & 0.608 & 0.608 & 0.601 \\
         parameter(d)                  & 1e-5& 50& 8  & 0.583 & 0.595 & 0.584 & 0.580 \\
         \rowcolor[rgb]{0.9,0.9,0}parameter(e)                  & 1e-5& 75& 16 & 0.583 & 0.611 & 0.590 & 0.581 \\
         parameter(f)                  & 5e-6& 100& 8 & 0.560 & 0.577 & 0.577 & 0.555 \\


    \end{tabular}

    }
\end{table}

% \subsection{Selecting hyperparameters and models}
\subsection{Examining Models and Parameters}
It turned out that the end-to-end model outperformed the context vectors approach.
Possible explanation is  that while the context vectors approach only uses the [CLS] token that is altered by the accompanying words,
the end-to-end approach makes use of the entire text.
Besides, context vectors are generated from a frozen model that is not specialized for this particular task,
whereas in the end-to-end approach, models are retrained and likely to be more task-specific.


\section{Q6- Conclusions and Future Work}
\subsection{Evaluating on Test Data}
The performance on the test set of the end-to-end model with the parameter(e) is shown in Table 10.
Also, the confusion matrix is shown in Figure 4.
\begin{table}[htbp]
    \centering
    \caption{Metrics on Test Set}
    \small
    \scalebox{1}[1]{
    \begin{tabular}{l|lll|cccc}
         Approach & Learning\_rate & Epochs & Batch\_size & Accuracy & Precision & Recall & F1 \\ \hline 
         end-to-end parameter(e)                  & 1e-5& 75& 16  & 0.597 & 0.617 & 0.592  & 0.591 \\
    \end{tabular}
    }
\end{table}

\begin{figure}[H]
  \begin{center}
  \includegraphics[width=120mm]{figures/confusion_mat_on_test.png}
  \caption{Confusion Matrix for Test Data}
  \end{center}
\end{figure}

\subsection{Manually Examining the Result}
(B)
Considering the confusion matrix, Dua Lipa has higher False Positive.
Also, more than half of False Negative for Lady Gaga is Dua lipa.
Therefore, misclassification of Lady Gaga into Dua Lipa and Flase Positive for Dua Lipa are considered.


First, the write of a song is checked for the first case, as it can be possible those songs are written by Dua Lipa.
However, those songs are actually written from Lady Gaga, while in some songs, several writers wrote them.

Next, it is hypothesized that lyrics are similar to Dua Lipa.
This was examined by taking the average document similarity with Jaccard Similarity
of every pair of misclassified songs and correctly classified songs.

Here, I write songs that are from "Lady Gaga" but classified as "Dua Lipa" as "Gaga\_Dua".
This means False Positive for "Dua lipa" and False Negative for "Lady Gaga".

The result showed that False Positives of Dua Lipa had higher Jaccard Similarity than 
true labels for each one as shown in Table (???).
This might explain the wrong classification, although this document similarity is not trustworthy enough.
Since True Positives have small similarities with themselves.

\begin{table}[htbp]
\centering
\caption{Accuracy Values}
\begin{tabular}{|c|l|c|}
\hline
\textbf{Intention} & \textbf{Pairs} & \textbf{Accuracy} \\ \hline
\multirow{5}{*}{False Negatives with True Labels} & Gaga\_Dua vs Gaga\_Gaga & 0.174 \\
& Taylor\_Dua vs Taylor\_Taylor & 0.209 \\
& Ariana\_Dua vs Ariana\_Ariana & 0.199 \\
& Selena\_Dua vs Selena\_Selena & 0.154 \\
& Jennifer\_Dua vs Jennifer\_Jennifer & 0.188 \\
\hline
\multirow{5}{*}{False Positives of Dua Lipa} & Gaga\_Dua vs Dua\_Dua & 0.205 \\
& Taylor\_Dua vs Dua\_Dua & 0.203 \\
& Ariana\_Dua vs Dua\_Dua & 0.206 \\
& Selena\_Dua vs Dua\_Dua & 0.194 \\
& Jennifer\_Dua vs Dua\_Dua & 0.208 \\
\hline
\multirow{7}{*}{True Positives against themselves} & Dua\_Dua vs Dua\_Dua & 0.196 \\
& Gaga\_Gaga vs Gaga\_Gaga & 0.158 \\
& Taylor\_Taylor vs Taylor\_Taylor & 0.207 \\
& Ariana\_Ariana vs Ariana\_Ariana & 0.179 \\
& Selena\_Selena vs Selena\_Selena & 0.116 \\
& Jennifer\_Jennifer vs Jennifer\_Jennifer & 0.164 \\
& XXX\_XXX vs XXX\_XXX & 0.128 \\
\hline
\multirow{5}{*}{True Positives against other True Positives} & Gaga\_Gaga vs Dua\_Dua & 0.164 \\
& Taylor\_Taylor vs Dua\_Dua & 0.195 \\
& Ariana\_Ariana vs Dua\_Dua & 0.183 \\
& Selena\_Selena vs Dua\_Dua & 0.148 \\
& Jennifer\_Jennifer vs Dua\_Dua & 0.169 \\
\hline
\end{tabular}
\label{tab:accuracy}
\end{table}







\subsection{Discussion on performance}
(C)
% The final performance of metrics were around 0.6.
% Precision was higher than recall, and so it can be interpreted the model does not cover the whole characteristics of a singer,
% but succeeded in capture the core feature of a singer.(あやしい)
% Especially for "Eminem", metrics are so high that recall is 33/34, and  precision is 33/38 and succeeded in classification. 

% confusion matの傾向が高い場合: (aをbとまちがえやすいだけ)
Since confusion matrix shows mistakes occured mainly with similar singers, it is successful in 
detecting singer characteristics. 
In order for singers to maintain their tren,or for recommendation system of music service.

% confusion matの傾向がない場合: (ランダムに間違える)
% Confusion matrix shows mistakes occured randomly, and thus the model fails to detect singer characteristics
% for about 40 percent. This performance is not high enough for singers to maintain their trend, so further
% improvements are needed.
However, for recommendation system of music service, where people are likely to be more generous to errornous recommendation,
this performance is high enough as it can show a song that fits a trend more frequently than a song that does not.

(D)
As for possible negative social effects of deployment, people might start to write a songs that 
are likely to be recommended, and thus harms the diversity of songs and people's negative attitude towards recommendation system.
Another possible case is that singers become reluctant to publish the songs that have different characteristics from their other songs,
afraid of being suspected stealing lyrics or change their style.
This also might leads to less diversity.

(E)
One step is making the data cleaner.
The lyrics used in this experiment is taken from the web and contain other data, such as 
html token, or translation data. Removing theese completely can improve the performance.
Another step is integrating data from other source as the year published, the number of the song covered by other singers, 
the singers' comments on a song.
Tuning hyperparameters thoroughly and also trying other models from "roberta\_base" might also result in better performance might also result in better performance.


(F)
40hours

\end{document}